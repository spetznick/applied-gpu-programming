{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2\n",
        "Below is an example that runs native CUDA code.\n",
        "\n",
        "1.   We investigate the CUDA version, drivers and the avaiable GPU with nvidia-smi and nvcc-version\n",
        "2.   We use the IPython magic command \"%%writefile filename\" to save a *.cu program\n",
        "3.   We then compile and run the *.cu program with nvcc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q0-ZomlNSrF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJlrROAn5Rqj",
        "outputId": "ecbba1a0-b743-4aa3-bbbd-7e4e8e65f752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Wed Nov 13 10:23:43 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Next, we write a native CUDA code and save it as 'vectorAdd.cu'\n"
      ],
      "metadata": {
        "id": "vsbr4brQH6v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vectorAdd.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "int main() {\n",
        "int a, b, c;\n",
        "// host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;\n",
        "// device copies of variables a, b & c\n",
        "int size = sizeof(int);\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "// Setup input values\n",
        "c = 0;\n",
        "a = 3;\n",
        "b = 5;\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "// Launch add() kernel on GPU\n",
        "add<<<1,1>>>(d_a, d_b, d_c);\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\"result is %d\\n\",c);\n",
        "// Cleanup\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0S5AUrl4eI8",
        "outputId": "719e1c72-bf57-4fd1-f150-cc6f1646c5ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw2_ex1.cu\n",
        "#include <stdio.h>\n",
        "#include <sys/time.h>\n",
        "#include <test>\n",
        "#include <>\n",
        "#include test\n",
        "\n",
        "#define DataType double\n",
        "\n",
        "__global__ void vecAdd(DataType *in1, DataType *in2, DataType *out, int len) {\n",
        "  //@@ Insert code to implement vector addition here\n",
        "}\n",
        "\n",
        "//@@ Insert code to implement timer start\n",
        "\n",
        "//@@ Insert code to implement timer stop\n",
        "\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "\n",
        "  int inputLength;\n",
        "  DataType *hostInput1;\n",
        "  DataType *hostInput2;\n",
        "  DataType *hostOutput;\n",
        "  DataType *resultRef;\n",
        "  DataType *deviceInput1;\n",
        "  DataType *deviceInput2;\n",
        "  DataType *deviceOutput;\n",
        "\n",
        "  //@@ Insert code below to read in inputLength from args\n",
        "\n",
        "\n",
        "  printf(\"The input length is %d\\n\", inputLength);\n",
        "\n",
        "  //@@ Insert code below to allocate Host memory for input and output\n",
        "\n",
        "\n",
        "  //@@ Insert code below to initialize hostInput1 and hostInput2 to random numbers, and create reference result in CPU\n",
        "\n",
        "\n",
        "  //@@ Insert code below to allocate GPU memory here\n",
        "\n",
        "\n",
        "  //@@ Insert code to below to Copy memory to the GPU here\n",
        "\n",
        "\n",
        "  //@@ Initialize the 1D grid and block dimensions here\n",
        "\n",
        "\n",
        "  //@@ Launch the GPU Kernel here\n",
        "\n",
        "\n",
        "  //@@ Copy the GPU memory back to the CPU here\n",
        "\n",
        "\n",
        "  //@@ Insert code below to compare the output with the reference\n",
        "\n",
        "\n",
        "  //@@ Free the GPU memory here\n",
        "\n",
        "  //@@ Free the CPU memory here\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CD-0ssP7iOX",
        "outputId": "4a38f310-47d5-47fd-f400-ec2341404c7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hw2_ex1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We compile the saved cuda code using nvcc compiler"
      ],
      "metadata": {
        "id": "TqdaBa9wIICn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc vectorAdd.cu -o vectorAdd\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvYps9NQ40NC",
        "outputId": "81e2e07d-f086-4893-a7a7-110d05d69e8c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hw2_ex1.cu  sample_data  vectorAdd  vectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, we execute the binary of the compiled code"
      ],
      "metadata": {
        "id": "SUALHJy9IPvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !./vectorAdd\n",
        "!nvprof --print-gpu-trace ./vectorAdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF-iISqy7O2E",
        "outputId": "75f5464b-76ab-4bba-bc71-498cf330fd8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2715== NVPROF is profiling process 2715, command: ./vectorAdd\n",
            "result is 8\n",
            "==2715== Profiling application: ./vectorAdd\n",
            "==2715== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "168.96ms  1.1840us                    -               -         -         -         -        4B  3.2219MB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "168.97ms     672ns                    -               -         -         -         -        4B  5.6766MB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "169.18ms  3.3600us              (1 1 1)         (1 1 1)        16        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  add(int*, int*, int*) [130]\n",
            "169.19ms  2.0800us                    -               -         -         -         -        4B  1.8340MB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu ./vectorAdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97UEqPicE2OA",
        "outputId": "0a7b40f5-807e-485f-bea8-6e5f7c283a0d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 3292 (/content/vectorAdd)\n",
            "==PROF== Profiling \"add(int *, int *, int *)\" - 0: 0%....50%....100% - 8 passes\n",
            "result is 8\n",
            "==PROF== Disconnected from process 3292\n",
            "[3292] vectorAdd@127.0.0.1\n",
            "  add(int *, int *, int *) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.63\n",
            "    SM Frequency            cycle/usecond       546.22\n",
            "    Elapsed Cycles                  cycle        1,993\n",
            "    Memory Throughput                   %         0.54\n",
            "    DRAM Throughput                     %         0.08\n",
            "    Duration                      usecond         3.65\n",
            "    L1/TEX Cache Throughput             %        19.98\n",
            "    L2 Cache Throughput                 %         0.54\n",
            "    SM Active Cycles                cycle        20.40\n",
            "    Compute (SM) Throughput             %         0.01\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                     1\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread               1\n",
            "    Waves Per SM                                                0.00\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 3.03%                                                                                      \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "    ----- --------------------------------------------------------------------------------------------------------------\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block          128\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block           32\n",
            "    Theoretical Active Warps per SM        warp           16\n",
            "    Theoretical Occupancy                     %           50\n",
            "    Achieved Occupancy                        %         3.12\n",
            "    Achieved Active Warps Per SM           warp            1\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 93.75%                                                                                     \n",
            "          This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM. This   \n",
            "          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory. The difference     \n",
            "          between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the result of warp       \n",
            "          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    \n",
            "          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    }
  ]
}