{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2\n",
        "Explain how the program is compiled and run.\n",
        "For a vector length of N:\n",
        "\n",
        "1. How many floating operations are being performed in your vector add kernel?\n",
        "2. How many global memory reads are being performed by your kernel?\n",
        "\n",
        "For a vector length of 512:\n",
        "\n",
        "1. Explain how many CUDA threads and thread blocks you used.\n",
        "2. Profile your program with Nvidia Nsight. What Achieved Occupancy did you get? You might find https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-metric-comparison\n",
        "Links to an external site. useful.\n",
        "\n",
        "Now increase the vector length to 262140:\n",
        "\n",
        "1. Did your program still work? If not, what changes did you make?\n",
        "2. Explain how many CUDA threads and thread blocks you used.\n",
        "Profile your program with Nvidia Nsight. What Achieved Occupancy do you get now?\n",
        "\n",
        "Further increase the vector length (try 6-10 different vector length), plot a stacked bar chart showing the breakdown of time including (1) data copy from host to device (2) the CUDA kernel (3) data copy from device to host. For this, you will need to add simple CPU timers to your code regions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q0-ZomlNSrF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJlrROAn5Rqj",
        "outputId": "98b53e63-a295-4764-d57a-79842f590866"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Wed Nov 13 13:38:48 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Next, we write a native CUDA code and save it as 'vectorAdd.cu'\n"
      ],
      "metadata": {
        "id": "vsbr4brQH6v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile vectorAdd.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "*c = *a + *b;\n",
        "}\n",
        "int main() {\n",
        "int a, b, c;\n",
        "// host copies of variables a, b & c\n",
        "int *d_a, *d_b, *d_c;\n",
        "// device copies of variables a, b & c\n",
        "int size = sizeof(int);\n",
        "// Allocate space for device copies of a, b, c\n",
        "cudaMalloc((void **)&d_a, size);\n",
        "cudaMalloc((void **)&d_b, size);\n",
        "cudaMalloc((void **)&d_c, size);\n",
        "// Setup input values\n",
        "c = 0;\n",
        "a = 3;\n",
        "b = 5;\n",
        "// Copy inputs to device\n",
        "cudaMemcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
        "// Launch add() kernel on GPU\n",
        "add<<<1,1>>>(d_a, d_b, d_c);\n",
        "// Copy result back to host\n",
        "cudaError err = cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "  if(err!=cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "printf(\"result is %d\\n\",c);\n",
        "// Cleanup\n",
        "cudaFree(d_a);\n",
        "cudaFree(d_b);\n",
        "cudaFree(d_c);\n",
        "return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0S5AUrl4eI8",
        "outputId": "246a46ff-7c03-44d0-9846-fc82ea8c0ba4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw2_ex1.cu\n",
        "#include <stdio.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define DataType double\n",
        "\n",
        "__global__ void vecAdd(DataType *in1, DataType *in2, DataType *out, int len) {\n",
        "  //@@ Insert code to implement vector addition here\n",
        "  int myID = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "  if (myID < len) {\n",
        "    out[myID] = in1[myID] + in2[myID];\n",
        "  }\n",
        "}\n",
        "\n",
        "//@@ Insert code to implement timer start\n",
        "\n",
        "//@@ Insert code to implement timer stop\n",
        "\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "  int inputLength;\n",
        "  DataType *hostInput1;\n",
        "  DataType *hostInput2;\n",
        "  DataType *hostOutput;\n",
        "  DataType *resultRef;\n",
        "  DataType *deviceInput1;\n",
        "  DataType *deviceInput2;\n",
        "  DataType *deviceOutput;\n",
        "\n",
        "  //@@ Insert code below to read in inputLength from args\n",
        "  if (argc < 2) {\n",
        "    printf(\"Please provide a number as a command-line argument.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "  char *end;\n",
        "  inputLength = strtol(argv[1], &end, 10);  // base 10 for decimal\n",
        "  if (*end != '\\0') {\n",
        "    printf(\"Invalid integer: %s\\n\", argv[1]);\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  printf(\"The input length is %d\\n\", inputLength);\n",
        "\n",
        "  //@@ Insert code below to allocate Host memory for input and output\n",
        "  int size = inputLength * sizeof(DataType);\n",
        "  hostInput1 = (double *)malloc(size);\n",
        "  hostInput2 = (double *)malloc(size);\n",
        "  hostOutput = (double *)malloc(size);\n",
        "  resultRef = (double *)malloc(size);\n",
        "\n",
        "  //@@ Insert code below to initialize hostInput1 and hostInput2 to random numbers, and create reference result in CPU\n",
        "  srand(time(NULL)); // use current time as seed for random generator\n",
        "  for(int i =0; i < inputLength; ++i) {\n",
        "    hostInput1[i] = static_cast<DataType>(rand())/RAND_MAX;\n",
        "    hostInput2[i] = static_cast<DataType>(rand())/RAND_MAX;\n",
        "  }\n",
        "\n",
        "  //@@ Insert code below to allocate GPU memory here\n",
        "  cudaMalloc(&deviceInput1, size);\n",
        "  cudaMalloc(&deviceInput2, size);\n",
        "  cudaMalloc(&deviceOutput, size);\n",
        "\n",
        "  //@@ Insert code to below to Copy memory to the GPU here\n",
        "  cudaMemcpy(deviceInput1, hostInput1, size, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(deviceInput2, hostInput2, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "  //@@ Initialize the 1D grid and block dimensions here\n",
        "  dim3 numThreadsPerBlock = 256;\n",
        "  dim3 numBlocks;\n",
        "  numBlocks.x = (inputLength + numThreadsPerBlock.x - 1)/numThreadsPerBlock.x;\n",
        "\n",
        "  // Create CUDA events\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  // Record the start event\n",
        "  cudaEventRecord(start, 0);\n",
        "\n",
        "  // Launch the kernel\n",
        "  vecAdd<<<gridSize, blockSize>>>(deviceInput1, deviceInput2, deviceOutput, inputLength);\n",
        "\n",
        "  // Record the stop event\n",
        "  cudaEventRecord(stop, 0);\n",
        "\n",
        "  // Wait for the stop event to complete\n",
        "  cudaEventSynchronize(stop);\n",
        "\n",
        "  // Calculate the elapsed time\n",
        "  float milliseconds = 0;\n",
        "  cudaEventElapsedTime(&milliseconds, start, stop);\n",
        "\n",
        "  printf(\"Kernel execution time: %.4f ms\\n\", milliseconds);\n",
        "\n",
        "  cudaError_t kernel_err = cudaGetLastError();\n",
        "  if (kernel_err != cudaSuccess) {\n",
        "    printf(\"Kernel launch error: %s\\n\", cudaGetErrorString(kernel_err));\n",
        "  }\n",
        "\n",
        "  //@@ Copy the GPU memory back to the CPU here\n",
        "  cudaError err = cudaMemcpy(hostOutput, deviceOutput, size, cudaMemcpyDeviceToHost);\n",
        "  if(err != cudaSuccess) {\n",
        "      printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "  }\n",
        "\n",
        "  //@@ Insert code below to compare the output with the reference\n",
        "  for(int i = 0; i < inputLength; ++i) {\n",
        "    resultRef[i] = hostInput1[i] + hostInput2[i];\n",
        "  }\n",
        "  for(int i = 0; i < inputLength; ++i) {\n",
        "    if (abs(resultRef[i] - hostOutput[i]) > 1e-5) {\n",
        "      printf(\"Addition wrong at %i: (cpu, gpu) = (%.3f, %.3f)\", i, resultRef[i], hostOutput[i]);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  //@@ Free the GPU memory here\n",
        "  cudaFree(deviceInput1);\n",
        "  cudaFree(deviceInput2);\n",
        "  cudaFree(deviceOutput);\n",
        "\n",
        "  //@@ Free the CPU memory here\n",
        "  free(hostInput1);\n",
        "  free(hostInput2);\n",
        "  free(hostOutput);\n",
        "  free(resultRef);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CD-0ssP7iOX",
        "outputId": "1a49d608-c88a-4a49-ac9e-2356778db9ac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hw2_ex1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We compile the saved cuda code using nvcc compiler"
      ],
      "metadata": {
        "id": "TqdaBa9wIICn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hw2_ex1.cu -o hw2_ex1\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvYps9NQ40NC",
        "outputId": "3939e43c-f90b-4a74-81ff-2a8c4e350827"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hw2_ex1  hw2_ex1.cu  sample_data  vectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, we execute the binary of the compiled code"
      ],
      "metadata": {
        "id": "SUALHJy9IPvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hw2_ex1 10"
      ],
      "metadata": {
        "id": "LHmpZNdmy_6-",
        "outputId": "0ff39b09-5a84-4123-ec02-eaf8885be931",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !./vectorAdd\n",
        "!nvprof --print-gpu-trace ./vectorAdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF-iISqy7O2E",
        "outputId": "75f5464b-76ab-4bba-bc71-498cf330fd8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2715== NVPROF is profiling process 2715, command: ./vectorAdd\n",
            "result is 8\n",
            "==2715== Profiling application: ./vectorAdd\n",
            "==2715== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "168.96ms  1.1840us                    -               -         -         -         -        4B  3.2219MB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "168.97ms     672ns                    -               -         -         -         -        4B  5.6766MB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "169.18ms  3.3600us              (1 1 1)         (1 1 1)        16        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  add(int*, int*, int*) [130]\n",
            "169.19ms  2.0800us                    -               -         -         -         -        4B  1.8340MB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu ./vectorAdd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97UEqPicE2OA",
        "outputId": "0a7b40f5-807e-485f-bea8-6e5f7c283a0d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 3292 (/content/vectorAdd)\n",
            "==PROF== Profiling \"add(int *, int *, int *)\" - 0: 0%....50%....100% - 8 passes\n",
            "result is 8\n",
            "==PROF== Disconnected from process 3292\n",
            "[3292] vectorAdd@127.0.0.1\n",
            "  add(int *, int *, int *) (1, 1, 1)x(1, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.63\n",
            "    SM Frequency            cycle/usecond       546.22\n",
            "    Elapsed Cycles                  cycle        1,993\n",
            "    Memory Throughput                   %         0.54\n",
            "    DRAM Throughput                     %         0.08\n",
            "    Duration                      usecond         3.65\n",
            "    L1/TEX Cache Throughput             %        19.98\n",
            "    L2 Cache Throughput                 %         0.54\n",
            "    SM Active Cycles                cycle        20.40\n",
            "    Compute (SM) Throughput             %         0.01\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                     1\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              16\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    Threads                                   thread               1\n",
            "    Waves Per SM                                                0.00\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Estimated Speedup: 3.03%                                                                                      \n",
            "          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 1      \n",
            "          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       \n",
            "          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      \n",
            "          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      \n",
            "          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     \n",
            "          kernels that frequently call __syncthreads(). See the Hardware Model                                          \n",
            "          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      \n",
            "          details on launch configurations.                                                                             \n",
            "    ----- --------------------------------------------------------------------------------------------------------------\n",
            "    OPT   Estimated Speedup: 97.5%                                                                                      \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block          128\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block           32\n",
            "    Theoretical Active Warps per SM        warp           16\n",
            "    Theoretical Occupancy                     %           50\n",
            "    Achieved Occupancy                        %         3.12\n",
            "    Achieved Active Warps Per SM           warp            1\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 93.75%                                                                                     \n",
            "          This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that can fit on the SM. This   \n",
            "          kernel's theoretical occupancy (50.0%) is limited by the required amount of shared memory. The difference     \n",
            "          between calculated theoretical (50.0%) and measured achieved occupancy (3.1%) can be the result of warp       \n",
            "          scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between    \n",
            "          warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide           \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    }
  ]
}