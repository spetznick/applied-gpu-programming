{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spetznick/applied-gpu-programming/blob/colab-s/assignment-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment 2\n",
        "Explain how the program is compiled and run.\n",
        "For a vector length of N:\n",
        "\n",
        "1. How many floating operations are being performed in your vector add kernel?\n",
        "2. How many global memory reads are being performed by your kernel?\n",
        "\n",
        "For a vector length of 512:\n",
        "\n",
        "1. Explain how many CUDA threads and thread blocks you used.\n",
        "2. Profile your program with Nvidia Nsight. What Achieved Occupancy did you get? You might find https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-metric-comparison\n",
        "Links to an external site. useful.\n",
        "\n",
        "Now increase the vector length to 262140:\n",
        "\n",
        "1. Did your program still work? If not, what changes did you make?\n",
        "2. Explain how many CUDA threads and thread blocks you used.\n",
        "Profile your program with Nvidia Nsight. What Achieved Occupancy do you get now?\n",
        "\n",
        "Further increase the vector length (try 6-10 different vector length), plot a stacked bar chart showing the breakdown of time including (1) data copy from host to device (2) the CUDA kernel (3) data copy from device to host. For this, you will need to add simple CPU timers to your code regions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q0-ZomlNSrF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJlrROAn5Rqj",
        "outputId": "98b53e63-a295-4764-d57a-79842f590866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Wed Nov 13 13:38:48 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## First exercise of the second assignment\n"
      ],
      "metadata": {
        "id": "vsbr4brQH6v3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw2_ex1.cu\n",
        "#include <stdio.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define DataType double\n",
        "\n",
        "__global__ void vecAdd(DataType *in1, DataType *in2, DataType *out, int len) {\n",
        "  int myID = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (myID < len) {\n",
        "    out[myID] = in1[myID] + in2[myID];\n",
        "  }\n",
        "}\n",
        "\n",
        "// Function to calculate mean and standard deviation of an array of floats\n",
        "void calculateMeanAndStdDev(float *times, int numRuns, float *mean, float *stdDev) {\n",
        "  float sum = 0.0;\n",
        "  for (int i = 0; i < numRuns; i++) {\n",
        "    sum += times[i];\n",
        "  }\n",
        "  *mean = sum / numRuns;\n",
        "\n",
        "  float variance = 0.0;\n",
        "  for (int i = 0; i < numRuns; i++) {\n",
        "    variance += (times[i] - *mean) * (times[i] - *mean);\n",
        "  }\n",
        "  *stdDev = sqrt(variance / numRuns);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "  if (argc < 4) {\n",
        "    printf(\"Please provide vector length, thread number, and number of runs as command-line arguments.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  int inputLength = strtol(argv[1], NULL, 10);\n",
        "  int numThreadsPerBlock = strtol(argv[2], NULL, 10);\n",
        "  int numRuns = strtol(argv[3], NULL, 10);\n",
        "\n",
        "  int size = inputLength * sizeof(DataType);\n",
        "  DataType *hostInput1 = (DataType *)malloc(size);\n",
        "  DataType *hostInput2 = (DataType *)malloc(size);\n",
        "  DataType *hostOutput = (DataType *)malloc(size);\n",
        "  DataType *resultRef = (DataType *)malloc(size);\n",
        "\n",
        "  // Initialize host inputs with random values\n",
        "  srand(time(NULL));\n",
        "  for (int i = 0; i < inputLength; ++i) {\n",
        "    hostInput1[i] = (DataType)rand() / RAND_MAX;\n",
        "    hostInput2[i] = (DataType)rand() / RAND_MAX;\n",
        "  }\n",
        "\n",
        "  DataType *deviceInput1, *deviceInput2, *deviceOutput;\n",
        "  cudaMalloc(&deviceInput1, size);\n",
        "  cudaMalloc(&deviceInput2, size);\n",
        "  cudaMalloc(&deviceOutput, size);\n",
        "\n",
        "  dim3 blockDim(numThreadsPerBlock);\n",
        "  dim3 gridDim((inputLength + blockDim.x - 1) / blockDim.x);\n",
        "  printf(\"The input length is %d, (numBlocks, numThreadsPerBlock) = (%d, %d).\\n\", inputLength, gridDim.x, blockDim.x);\n",
        "\n",
        "  // CUDA events for timing\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  float *kernelTimes = (float *)malloc(numRuns * sizeof(float));\n",
        "  float *copyToDeviceTimes = (float *)malloc(numRuns * sizeof(float));\n",
        "  float *copyToHostTimes = (float *)malloc(numRuns * sizeof(float));\n",
        "\n",
        "  for (int run = 0; run < numRuns; run++) {\n",
        "    // Time data copy to device\n",
        "    cudaEventRecord(start);\n",
        "    cudaMemcpy(deviceInput1, hostInput1, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(deviceInput2, hostInput2, size, cudaMemcpyHostToDevice);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&copyToDeviceTimes[run], start, stop);\n",
        "\n",
        "    // Time kernel execution\n",
        "    cudaEventRecord(start);\n",
        "    vecAdd<<<gridDim, blockDim>>>(deviceInput1, deviceInput2, deviceOutput, inputLength);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&kernelTimes[run], start, stop);\n",
        "\n",
        "    // Time data copy back to host\n",
        "    cudaEventRecord(start);\n",
        "    cudaMemcpy(hostOutput, deviceOutput, size, cudaMemcpyDeviceToHost);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&copyToHostTimes[run], start, stop);\n",
        "  }\n",
        "\n",
        "  float meanKernelTime, stdDevKernelTime;\n",
        "  float meanCopyToDeviceTime, stdDevCopyToDeviceTime;\n",
        "  float meanCopyToHostTime, stdDevCopyToHostTime;\n",
        "\n",
        "  calculateMeanAndStdDev(kernelTimes, numRuns, &meanKernelTime, &stdDevKernelTime);\n",
        "  calculateMeanAndStdDev(copyToDeviceTimes, numRuns, &meanCopyToDeviceTime, &stdDevCopyToDeviceTime);\n",
        "  calculateMeanAndStdDev(copyToHostTimes, numRuns, &meanCopyToHostTime, &stdDevCopyToHostTime);\n",
        "\n",
        "  printf(\"Input length %d, (numBlocks, numThreadsPerBlock) = (%d, %d).\\n\", inputLength, gridDim.x, blockDim.x);\n",
        "  printf(\"Average kernel execution time of %d runs: %.4f +- %.4f ms\\n\", numRuns, meanKernelTime, stdDevKernelTime);\n",
        "  printf(\"Average time for data copy to device:     %.4f +- %.4f ms\\n\", meanCopyToDeviceTime, stdDevCopyToDeviceTime);\n",
        "  printf(\"Average time for data copy to host:       %.4f +- %.4f ms\\n\", meanCopyToHostTime, stdDevCopyToHostTime);\n",
        "\n",
        "  // Error checking and final result comparison\n",
        "  for (int i = 0; i < inputLength; ++i) {\n",
        "    resultRef[i] = hostInput1[i] + hostInput2[i];\n",
        "  }\n",
        "  for (int i = 0; i < inputLength; ++i) {\n",
        "    if (abs(resultRef[i] - hostOutput[i]) > 1e-5) {\n",
        "      printf(\"Addition wrong at %i: (cpu, gpu) = (%.3f, %.3f)\\n\", i, resultRef[i], hostOutput[i]);\n",
        "      break;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  cudaFree(deviceInput1);\n",
        "  cudaFree(deviceInput2);\n",
        "  cudaFree(deviceOutput);\n",
        "\n",
        "  free(hostInput1);\n",
        "  free(hostInput2);\n",
        "  free(hostOutput);\n",
        "  free(resultRef);\n",
        "  free(kernelTimes);\n",
        "  free(copyToDeviceTimes);\n",
        "  free(copyToHostTimes);\n",
        "\n",
        "  cudaEventDestroy(start);\n",
        "  cudaEventDestroy(stop);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qa0R2zhEGmL",
        "outputId": "282c2f07-37db-45ac-a1e7-7f9de234843f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting hw2_ex1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We compile the saved cuda code using nvcc compiler"
      ],
      "metadata": {
        "id": "TqdaBa9wIICn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc hw2_ex1.cu -o hw2_ex1\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvYps9NQ40NC",
        "outputId": "95f9fa15-d955-4918-bd1e-11d44b8ff040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hw2_ex1  hw2_ex1.cu  sample_data  vectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finally, we execute the binary of the compiled code"
      ],
      "metadata": {
        "id": "SUALHJy9IPvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hw2_ex1 10 256\n",
        "!./hw2_ex1 10 512\n",
        "!./hw2_ex1 512 256 10\n",
        "!./hw2_ex1 512 512 10\n",
        "!./hw2_ex1 512 1024 10\n",
        "!./hw2_ex1 10000 256\n",
        "!./hw2_ex1 10000 512\n",
        "!./hw2_ex1 10000 1024\n",
        "!./hw2_ex1 100000 256 10\n",
        "!./hw2_ex1 100000 512 10\n",
        "!./hw2_ex1 100000 1024 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHmpZNdmy_6-",
        "outputId": "5ec09f64-8ad9-4c4f-90fa-ecf9a3b170b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 512, (numBlocks, numThreadsPerBlock) = (2, 256).\n",
            "Input length 512, (numBlocks, numThreadsPerBlock) = (2, 256).\n",
            "Average kernel execution time of 10 runs: 0.0263 +- 0.0593 ms\n",
            "Average time for data copy to device:     0.0205 +- 0.0048 ms\n",
            "Average time for data copy to host:       0.0159 +- 0.0035 ms\n",
            "The input length is 512, (numBlocks, numThreadsPerBlock) = (1, 512).\n",
            "Input length 512, (numBlocks, numThreadsPerBlock) = (1, 512).\n",
            "Average kernel execution time of 10 runs: 0.0266 +- 0.0589 ms\n",
            "Average time for data copy to device:     0.0203 +- 0.0037 ms\n",
            "Average time for data copy to host:       0.0178 +- 0.0028 ms\n",
            "The input length is 512, (numBlocks, numThreadsPerBlock) = (1, 1024).\n",
            "Input length 512, (numBlocks, numThreadsPerBlock) = (1, 1024).\n",
            "Average kernel execution time of 10 runs: 0.0257 +- 0.0564 ms\n",
            "Average time for data copy to device:     0.0196 +- 0.0026 ms\n",
            "Average time for data copy to host:       0.0141 +- 0.0042 ms\n",
            "The input length is 100000, (numBlocks, numThreadsPerBlock) = (391, 256).\n",
            "Input length 100000, (numBlocks, numThreadsPerBlock) = (391, 256).\n",
            "Average kernel execution time of 10 runs: 0.0319 +- 0.0469 ms\n",
            "Average time for data copy to device:     0.4725 +- 0.0358 ms\n",
            "Average time for data copy to host:       0.3183 +- 0.1151 ms\n",
            "The input length is 100000, (numBlocks, numThreadsPerBlock) = (196, 512).\n",
            "Input length 100000, (numBlocks, numThreadsPerBlock) = (196, 512).\n",
            "Average kernel execution time of 10 runs: 0.0341 +- 0.0510 ms\n",
            "Average time for data copy to device:     0.4788 +- 0.0297 ms\n",
            "Average time for data copy to host:       0.3434 +- 0.1322 ms\n",
            "The input length is 100000, (numBlocks, numThreadsPerBlock) = (98, 1024).\n",
            "Input length 100000, (numBlocks, numThreadsPerBlock) = (98, 1024).\n",
            "Average kernel execution time of 10 runs: 0.0352 +- 0.0529 ms\n",
            "Average time for data copy to device:     0.4689 +- 0.0326 ms\n",
            "Average time for data copy to host:       0.3081 +- 0.1146 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector length 512"
      ],
      "metadata": {
        "id": "0M6oNMWvDBwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --section \"Occupancy\" ./hw2_ex1 512 256 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97UEqPicE2OA",
        "outputId": "d2d934ea-c197-441c-beb6-571c689a2274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 29549 (/content/hw2_ex1)\n",
            "The input length is 512, (numBlocks, numThreadsPerBlock) = (2, 256).\n",
            "==PROF== Profiling \"vecAdd\" - 0: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 1: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 2: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 3: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 4: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 5: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 6: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 7: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 8: 0%....50%....100% - 1 pass\n",
            "==PROF== Profiling \"vecAdd\" - 9: 0%....50%....100% - 1 pass\n",
            "Input length 512, (numBlocks, numThreadsPerBlock) = (2, 256).\n",
            "Average kernel execution time of 10 runs: 34.8490 +- 68.9776 ms\n",
            "Average time for data copy to device:     0.0283 +- 0.0080 ms\n",
            "Average time for data copy to host:       0.0331 +- 0.0096 ms\n",
            "==PROF== Disconnected from process 29549\n",
            "[29549] hw2_ex1@127.0.0.1\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.93\n",
            "    Achieved Active Warps Per SM           warp         7.66\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.07%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.9%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.42\n",
            "    Achieved Active Warps Per SM           warp         7.49\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.58%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.42\n",
            "    Achieved Active Warps Per SM           warp         7.49\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.58%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.43\n",
            "    Achieved Active Warps Per SM           warp         7.50\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.57%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.40\n",
            "    Achieved Active Warps Per SM           warp         7.49\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.6%                                                                                      \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.42\n",
            "    Achieved Active Warps Per SM           warp         7.50\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.58%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.41\n",
            "    Achieved Active Warps Per SM           warp         7.49\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.59%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.43\n",
            "    Achieved Active Warps Per SM           warp         7.50\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.57%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.42\n",
            "    Achieved Active Warps Per SM           warp         7.49\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.58%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "  vecAdd(double *, double *, double *, int) (2, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        23.43\n",
            "    Achieved Active Warps Per SM           warp         7.50\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 76.57%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (23.4%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector length 262140"
      ],
      "metadata": {
        "id": "Tx7958UODHoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --section \"Occupancy\" ./hw2_ex1 262140 256 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fesFqoAj9qRJ",
        "outputId": "562c51c5-4870-4306-e24a-96cb00254dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 29683 (/content/hw2_ex1)\n",
            "The input length is 262140, (numBlocks, numThreadsPerBlock) = (1024, 256).\n",
            "==PROF== Profiling \"vecAdd\" - 0: 0%....50%....100% - 1 pass\n",
            "Input length 262140, (numBlocks, numThreadsPerBlock) = (1024, 256).\n",
            "Average kernel execution time of 1 runs: 240.0350 +- 0.0000 ms\n",
            "Average time for data copy to device:     1.2110 +- 0.0000 ms\n",
            "Average time for data copy to host:       1.8493 +- 0.0000 ms\n",
            "==PROF== Disconnected from process 29683\n",
            "[29683] hw2_ex1@127.0.0.1\n",
            "  vecAdd(double *, double *, double *, int) (1024, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block           16\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            4\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        82.45\n",
            "    Achieved Active Warps Per SM           warp        26.39\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Estimated Speedup: 17.55%                                                                                     \n",
            "          This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated     \n",
            "          theoretical (100.0%) and measured achieved occupancy (82.5%) can be the result of warp scheduling overheads   \n",
            "          or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block    \n",
            "          as well as across blocks of the same kernel. See the CUDA Best Practices Guide                                \n",
            "          (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on           \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment with other sizes"
      ],
      "metadata": {
        "id": "MZbex8eZFwIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./hw2_ex1 100000 256 10\n",
        "!./hw2_ex1 1000000 256 10\n",
        "!./hw2_ex1 10000000 256 10\n",
        "!./hw2_ex1 100000000 256 10\n",
        "!./hw2_ex1 500000000 256 10\n",
        "!./hw2_ex1 1000000000 256 10\n",
        "!./hw2_ex1 10000000000 256 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knaNKoDu6FVS",
        "outputId": "a8e04439-c938-49be-b270-9d17b5cf9729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The input length is 100000, (numBlocks, numThreadsPerBlock) = (391, 256).\n",
            "Input length 100000, (numBlocks, numThreadsPerBlock) = (391, 256).\n",
            "Average kernel execution time of 10 runs: 0.0338 +- 0.0579 ms\n",
            "Average time for data copy to device:     0.4606 +- 0.0284 ms\n",
            "Average time for data copy to host:       0.3056 +- 0.1264 ms\n",
            "The input length is 1000000, (numBlocks, numThreadsPerBlock) = (3907, 256).\n",
            "Input length 1000000, (numBlocks, numThreadsPerBlock) = (3907, 256).\n",
            "Average kernel execution time of 10 runs: 0.1145 +- 0.0502 ms\n",
            "Average time for data copy to device:     3.2693 +- 0.1583 ms\n",
            "Average time for data copy to host:       2.1796 +- 1.2151 ms\n",
            "The input length is 10000000, (numBlocks, numThreadsPerBlock) = (39063, 256).\n",
            "Input length 10000000, (numBlocks, numThreadsPerBlock) = (39063, 256).\n",
            "Average kernel execution time of 10 runs: 0.9398 +- 0.0547 ms\n",
            "Average time for data copy to device:     34.1295 +- 0.7819 ms\n",
            "Average time for data copy to host:       21.0678 +- 11.7128 ms\n",
            "The input length is 100000000, (numBlocks, numThreadsPerBlock) = (390625, 256).\n",
            "Input length 100000000, (numBlocks, numThreadsPerBlock) = (390625, 256).\n",
            "Average kernel execution time of 10 runs: 9.1669 +- 0.0574 ms\n",
            "Average time for data copy to device:     349.7391 +- 15.5713 ms\n",
            "Average time for data copy to host:       213.4426 +- 116.5841 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def parse_results(results_str):\n",
        "    \"\"\"\n",
        "    Parses the results of multiple test runs and returns a DataFrame.\n",
        "\n",
        "    Args:\n",
        "    - results_str (str): A string containing the test run output.\n",
        "\n",
        "    Returns:\n",
        "    - df (pd.DataFrame): A DataFrame containing the parsed results.\n",
        "    \"\"\"\n",
        "\n",
        "    # Regex pattern to match each block of results for a single input length\n",
        "    pattern = r\"The input length is (\\d+), \\(numBlocks, numThreadsPerBlock\\) = \\(\\d+, \\d+\\).\\s*Input length \\d+, \\(numBlocks, numThreadsPerBlock\\) = \\(\\d+, \\d+\\).\\s*Average kernel execution time of \\d+ runs: ([\\d.]+) \\+- [\\d.]+ ms\\s*Average time for data copy to device: +([\\d.]+) \\+- [\\d.]+ ms\\s*Average time for data copy to host: +([\\d.]+) \\+- [\\d.]+ ms\"\n",
        "\n",
        "    # Use regex to find all matches in the string\n",
        "    matches = re.findall(pattern, results_str)\n",
        "\n",
        "    # Create a list to store the parsed data\n",
        "    data = []\n",
        "\n",
        "    # Parse each match and store as a dictionary\n",
        "    for match in matches:\n",
        "        input_length = int(match[0])\n",
        "        kernel_time = float(match[1])\n",
        "        copy_to_device_time = float(match[2])\n",
        "        copy_to_host_time = float(match[3])\n",
        "\n",
        "        # Append the parsed data to the list\n",
        "        data.append({\n",
        "            \"Input Length\": input_length,\n",
        "            \"Kernel Execution Time (ms)\": kernel_time,\n",
        "            \"Copy to Device Time (ms)\": copy_to_device_time,\n",
        "            \"Copy to Host Time (ms)\": copy_to_host_time\n",
        "        })\n",
        "\n",
        "    # Convert the data list to a DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage with the input printout string\n",
        "results_str = \"\"\"\n",
        "The input length is 100000, (numBlocks, numThreadsPerBlock) = (391, 256).\n",
        "Input length 100000, (numBlocks, numThreadsPerBlock) = (391, 256).\n",
        "Average kernel execution time of 10 runs: 0.0338 +- 0.0579 ms\n",
        "Average time for data copy to device:     0.4606 +- 0.0284 ms\n",
        "Average time for data copy to host:       0.3056 +- 0.1264 ms\n",
        "The input length is 1000000, (numBlocks, numThreadsPerBlock) = (3907, 256).\n",
        "Input length 1000000, (numBlocks, numThreadsPerBlock) = (3907, 256).\n",
        "Average kernel execution time of 10 runs: 0.1145 +- 0.0502 ms\n",
        "Average time for data copy to device:     3.2693 +- 0.1583 ms\n",
        "Average time for data copy to host:       2.1796 +- 1.2151 ms\n",
        "The input length is 10000000, (numBlocks, numThreadsPerBlock) = (39063, 256).\n",
        "Input length 10000000, (numBlocks, numThreadsPerBlock) = (39063, 256).\n",
        "Average kernel execution time of 10 runs: 0.9398 +- 0.0547 ms\n",
        "Average time for data copy to device:     34.1295 +- 0.7819 ms\n",
        "Average time for data copy to host:       21.0678 +- 11.7128 ms\n",
        "The input length is 100000000, (numBlocks, numThreadsPerBlock) = (390625, 256).\n",
        "Input length 100000000, (numBlocks, numThreadsPerBlock) = (390625, 256).\n",
        "Average kernel execution time of 10 runs: 9.1669 +- 0.0574 ms\n",
        "Average time for data copy to device:     349.7391 +- 15.5713 ms\n",
        "Average time for data copy to host:       213.4426 +- 116.5841 ms\n",
        "\"\"\"\n",
        "\n",
        "# Parse the results\n",
        "df = parse_results(results_str)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df)\n",
        "\n",
        "# # Data preparation\n",
        "# data = {\n",
        "#     \"Input Length\": [100000, 1000000, 5000000, 10000000, 50000000, 100000000],\n",
        "#     \"Kernel Execution Time (ms)\": [0.0332, 0.1134, 0.4788, 0.9397, 4.5879, 9.1720],\n",
        "#     \"Copy to Device Time (ms)\": [0.4653, 3.2890, 16.9438, 34.2606, 171.6111, 335.4579],\n",
        "#     \"Copy to Host Time (ms)\": [0.3076, 2.1924, 10.8117, 21.0833, 104.4536, 207.1870]\n",
        "# }\n",
        "\n",
        "# # Convert the data into a pandas DataFrame\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# Reshape the DataFrame for a stacked bar plot\n",
        "df_melted = df.melt(id_vars=\"Input Length\",\n",
        "                    value_vars=[\"Kernel Execution Time (ms)\", \"Copy to Device Time (ms)\", \"Copy to Host Time (ms)\"],\n",
        "                    var_name=\"Operation\",\n",
        "                    value_name=\"Time (ms)\")\n",
        "\n",
        "# Plotting the stacked bar plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_melted, x=\"Input Length\", y=\"Time (ms)\", hue=\"Operation\", dodge=False)\n",
        "\n",
        "# Adding plot details\n",
        "plt.title(\"Average Times for Data Copy and Kernel Execution\")\n",
        "plt.xlabel(\"Input Length\")\n",
        "plt.ylabel(\"Average Time (ms)\")\n",
        "plt.yscale(\"log\")  # Log scale can help better visualize the differences across large ranges\n",
        "plt.legend(title=\"Operation\", loc=\"upper left\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "Zn8Bi32pGR3L",
        "outputId": "3224f34d-451d-4478-e769-c47e995df5ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Input Length  Kernel Execution Time (ms)  Copy to Device Time (ms)  \\\n",
            "0        100000                      0.0338                    0.4606   \n",
            "1       1000000                      0.1145                    3.2693   \n",
            "2      10000000                      0.9398                   34.1295   \n",
            "3     100000000                      9.1669                  349.7391   \n",
            "\n",
            "   Copy to Host Time (ms)  \n",
            "0                  0.3056  \n",
            "1                  2.1796  \n",
            "2                 21.0678  \n",
            "3                213.4426  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nvidia_matrixmultiply.cu\n",
        "// Matrices are stored in row-major order:\n",
        "// M(row, col) = *(M.elements + row * M.stride + col)\n",
        "typedef struct {\n",
        "    int width;\n",
        "    int height;\n",
        "    int stride;\n",
        "    float* elements;\n",
        "} Matrix;\n",
        "// Get a matrix element\n",
        "__device__ float GetElement(const Matrix A, int row, int col)\n",
        "{\n",
        "    return A.elements[row * A.stride + col];\n",
        "}\n",
        "// Set a matrix element\n",
        "__device__ void SetElement(Matrix A, int row, int col,\n",
        "                           float value)\n",
        "{\n",
        "    A.elements[row * A.stride + col] = value;\n",
        "}\n",
        "// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is\n",
        "// located col sub-matrices to the right and row sub-matrices down\n",
        "// from the upper-left corner of A\n",
        " __device__ Matrix GetSubMatrix(Matrix A, int row, int col)\n",
        "{\n",
        "    Matrix Asub;\n",
        "    Asub.width    = BLOCK_SIZE;\n",
        "    Asub.height   = BLOCK_SIZE;\n",
        "    Asub.stride   = A.stride;\n",
        "    Asub.elements = &A.elements[A.stride * BLOCK_SIZE * row\n",
        "                                         + BLOCK_SIZE * col];\n",
        "    return Asub;\n",
        "}\n",
        "// Thread block size\n",
        "#define BLOCK_SIZE 16\n",
        "// Forward declaration of the matrix multiplication kernel\n",
        "__global__ void MatMulKernel(const Matrix, const Matrix, Matrix);\n",
        "// Matrix multiplication - Host code\n",
        "// Matrix dimensions are assumed to be multiples of BLOCK_SIZE\n",
        "void MatMul(const Matrix A, const Matrix B, Matrix C)\n",
        "{\n",
        "    // Load A and B to device memory\n",
        "    Matrix d_A;\n",
        "    d_A.width = d_A.stride = A.width; d_A.height = A.height;\n",
        "    size_t size = A.width * A.height * sizeof(float);\n",
        "    cudaMalloc(&d_A.elements, size);\n",
        "    cudaMemcpy(d_A.elements, A.elements, size,\n",
        "               cudaMemcpyHostToDevice);\n",
        "    Matrix d_B;\n",
        "    d_B.width = d_B.stride = B.width; d_B.height = B.height;\n",
        "    size = B.width * B.height * sizeof(float);\n",
        "    cudaMalloc(&d_B.elements, size);\n",
        "    cudaMemcpy(d_B.elements, B.elements, size,\n",
        "    cudaMemcpyHostToDevice);\n",
        "    // Allocate C in device memory\n",
        "    Matrix d_C;\n",
        "    d_C.width = d_C.stride = C.width; d_C.height = C.height;\n",
        "    size = C.width * C.height * sizeof(float);\n",
        "    cudaMalloc(&d_C.elements, size);\n",
        "    // Invoke kernel\n",
        "    dim3 dimBlock(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);\n",
        "    MatMulKernel<<<dimGrid, dimBlock>>>(d_A, d_B, d_C);\n",
        "    // Read C from device memory\n",
        "    cudaMemcpy(C.elements, d_C.elements, size,\n",
        "               cudaMemcpyDeviceToHost);\n",
        "    // Free device memory\n",
        "    cudaFree(d_A.elements);\n",
        "    cudaFree(d_B.elements);\n",
        "    cudaFree(d_C.elements);\n",
        "}\n",
        "// Matrix multiplication kernel called by MatMul()\n",
        " __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)\n",
        "{\n",
        "    // Block row and column\n",
        "    int blockRow = blockIdx.y;\n",
        "    int blockCol = blockIdx.x;\n",
        "    // Each thread block computes one sub-matrix Csub of C\n",
        "    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);\n",
        "    // Each thread computes one element of Csub\n",
        "    // by accumulating results into Cvalue\n",
        "    float Cvalue = 0;\n",
        "    // Thread row and column within Csub\n",
        "    int row = threadIdx.y;\n",
        "    int col = threadIdx.x;\n",
        "    // Loop over all the sub-matrices of A and B that are\n",
        "    // required to compute Csub\n",
        "    // Multiply each pair of sub-matrices together\n",
        "    // and accumulate the results\n",
        "    for (int m = 0; m < (A.width / BLOCK_SIZE); ++m) {\n",
        "        // Get sub-matrix Asub of A\n",
        "        Matrix Asub = GetSubMatrix(A, blockRow, m);\n",
        "        // Get sub-matrix Bsub of B\n",
        "        Matrix Bsub = GetSubMatrix(B, m, blockCol);\n",
        "        // Shared memory used to store Asub and Bsub respectively\n",
        "        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
        "        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
        "        // Load Asub and Bsub from device memory to shared memory\n",
        "        // Each thread loads one element of each sub-matrix\n",
        "        As[row][col] = GetElement(Asub, row, col);\n",
        "        Bs[row][col] = GetElement(Bsub, row, col);\n",
        "        // Synchronize to make sure the sub-matrices are loaded\n",
        "        // before starting the computation\n",
        "        __syncthreads();\n",
        "        // Multiply Asub and Bsub together\n",
        "        for (int e = 0; e < BLOCK_SIZE; ++e)\n",
        "            Cvalue += As[row][e] * Bs[e][col];\n",
        "        // Synchronize to make sure that the preceding\n",
        "        // computation is done before loading two new\n",
        "        // sub-matrices of A and B in the next iteration\n",
        "        __syncthreads();\n",
        "    }\n",
        "    // Write Csub to device memory\n",
        "    // Each thread writes one element\n",
        "    SetElement(Csub, row, col, Cvalue);\n",
        "}"
      ],
      "metadata": {
        "id": "NYvDvWH6tHOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile hw2_ex2.cu\n",
        "#include <stdio.h>\n",
        "#include <sys/time.h>\n",
        "\n",
        "#define DataType double\n",
        "\n",
        "// Compute C = A * B each thread computes a single entry\n",
        "__global__ void gemm(DataType *A, DataType *B, DataType *C, int numARows,\n",
        "                      int numAColumns, int numBRows, int numBColumns){\n",
        "  //@@ Insert code to implement matrix multiplication here\n",
        "  DataType cValue = 0;\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (row < numARows && col < numBColumns) {\n",
        "    for (int i = 0; i < numAColumns; ++i) {\n",
        "      cValue += A[row * numAColumns + i] * B[i * numBColumns + col];\n",
        "    }\n",
        "  }\n",
        "  C[row * numBColumns + col] = cValue;\n",
        "}\n",
        "\n",
        "// Function to calculate mean and standard deviation of an array of floats\n",
        "void calculateMeanAndStdDev(float *times, int numRuns, float *mean, float *stdDev) {\n",
        "  float sum = 0.0;\n",
        "  for (int i = 0; i < numRuns; i++) {\n",
        "    sum += times[i];\n",
        "  }\n",
        "  *mean = sum / numRuns;\n",
        "\n",
        "  float variance = 0.0;\n",
        "  for (int i = 0; i < numRuns; i++) {\n",
        "    variance += (times[i] - *mean) * (times[i] - *mean);\n",
        "  }\n",
        "  *stdDev = sqrt(variance / numRuns);\n",
        "}\n",
        "\n",
        "int main(int argc, char **argv) {\n",
        "\n",
        "  DataType *hostA; // The A matrix\n",
        "  DataType *hostB; // The B matrix\n",
        "  DataType *hostC; // The output C matrix\n",
        "  DataType *resultRef; // The reference result\n",
        "  DataType *deviceA;\n",
        "  DataType *deviceB;\n",
        "  DataType *deviceC;\n",
        "  int numARows;    // number of rows in the matrix A\n",
        "  int numAColumns; // number of columns in the matrix A\n",
        "  int numBRows;    // number of rows in the matrix B\n",
        "  int numBColumns; // number of columns in the matrix B\n",
        "  int numCRows;\n",
        "  int numCColumns;\n",
        "\n",
        "  //@@ Insert code below to read in numARows, numAColumns, numBColumns from args\n",
        "  if (argc < 6) {\n",
        "    printf(\"Please provide number of matrix rows and columns of A and B, respectively, and the number of runs as command-line arguments.\\n\");\n",
        "    return 1;\n",
        "  }\n",
        "\n",
        "  int numARows = strtol(argv[1], NULL, 15);\n",
        "  int numAColumns = strtol(argv[2], NULL, 15);\n",
        "  int numBRows = strtol(argv[3], NULL, 15);\n",
        "  int numBColumns = strtol(argv[4], NULL, 15);\n",
        "  int numRuns = strtol(argv[5], NULL, 10);\n",
        "  numCRows = numARows;\n",
        "  numCColumns = numBColumns;\n",
        "\n",
        "  //! Input dim is given as expected by user but internally, B matrix is stored in column-major\n",
        "  printf(\"Input matrix dim (%d x %d) (%d x %d) (%d x %d)\\n\", numARows, numAColumns, numBRows, numBColumns, numCRows, numCColumns);\n",
        "\n",
        "\n",
        "  //@@ Insert code below to allocate Host memory for input and output\n",
        "  int size = numAColumns * numARows * sizeof(DataType);\n",
        "  DataType *hostA = (DataType *)malloc(size);\n",
        "  DataType *hostB = (DataType *)malloc(size);\n",
        "  DataType *hostC = (DataType *)malloc(size);\n",
        "  DataType *resultRef = (DataType *)malloc(size);\n",
        "\n",
        "\n",
        "  //@@ Insert code below to initialize hostA and hostB to random numbers, and create reference result in CPU\n",
        "  srand(time(NULL));\n",
        "  for (int i = 0; i < inputLength; ++i) {\n",
        "    hostA[i] = (DataType)rand() / RAND_MAX;\n",
        "    hostB[i] = (DataType)rand() / RAND_MAX;\n",
        "  }\n",
        "\n",
        "  //@@ Insert code below to allocate GPU memory here\n",
        "  cudaMalloc(&deviceA, size);\n",
        "  cudaMalloc(&deviceB, size);\n",
        "  cudaMalloc(&deviceC, size);\n",
        "\n",
        "  //@@ Initialize the grid and block dimensions here\n",
        "  // 16x16 should be possible for most architectures\n",
        "  dim3 blockDim(16, 16);\n",
        "  dim3 gridDim(16, 16);\n",
        "  dim3 dimGrid(B.width / dimBlock.x, A.height / dimBlock.y);\n",
        "\n",
        "  // CUDA events for timing\n",
        "  cudaEvent_t start, stop;\n",
        "  cudaEventCreate(&start);\n",
        "  cudaEventCreate(&stop);\n",
        "\n",
        "  float *kernelTimes = (float *)malloc(numRuns * sizeof(float));\n",
        "  float *copyToDeviceTimes = (float *)malloc(numRuns * sizeof(float));\n",
        "  float *copyToHostTimes = (float *)malloc(numRuns * sizeof(float));\n",
        "\n",
        "  for (int run = 0; run < numRuns; run++) {\n",
        "    //@@ Insert code to below to Copy memory to the GPU here\n",
        "    // Time data copy to device\n",
        "    cudaEventRecord(start);\n",
        "    cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&copyToDeviceTimes[run], start, stop);\n",
        "\n",
        "    //@@ Launch the GPU Kernel here\n",
        "    cudaEventRecord(start);\n",
        "    gemm<<<gridDim, blockDim>>>(deviceA, deviceB, deviceC, numARows,\n",
        "                      numAColumns, numBRows, numBColumns);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&kernelTimes[run], start, stop);\n",
        "\n",
        "    //@@ Copy the GPU memory back to the CPU here\n",
        "    cudaEventRecord(start);\n",
        "    cudaMemcpy(hostOutput, deviceOutput, size, cudaMemcpyDeviceToHost);\n",
        "    cudaEventRecord(stop);\n",
        "    cudaEventSynchronize(stop);\n",
        "    cudaEventElapsedTime(&copyToHostTimes[run], start, stop);\n",
        "  }\n",
        "\n",
        "  float meanKernelTime, stdDevKernelTime;\n",
        "  float meanCopyToDeviceTime, stdDevCopyToDeviceTime;\n",
        "  float meanCopyToHostTime, stdDevCopyToHostTime;\n",
        "\n",
        "  calculateMeanAndStdDev(kernelTimes, numRuns, &meanKernelTime, &stdDevKernelTime);\n",
        "  calculateMeanAndStdDev(copyToDeviceTimes, numRuns, &meanCopyToDeviceTime, &stdDevCopyToDeviceTime);\n",
        "  calculateMeanAndStdDev(copyToHostTimes, numRuns, &meanCopyToHostTime, &stdDevCopyToHostTime);\n",
        "\n",
        "  printf(\"Average kernel execution time of %d runs: %.4f +- %.4f ms\\n\", numRuns, meanKernelTime, stdDevKernelTime);\n",
        "  printf(\"Average time for data copy to device:     %.4f +- %.4f ms\\n\", meanCopyToDeviceTime, stdDevCopyToDeviceTime);\n",
        "  printf(\"Average time for data copy to host:       %.4f +- %.4f ms\\n\", meanCopyToHostTime, stdDevCopyToHostTime);\n",
        "\n",
        "  //@@ Insert code below to compare the output with the reference\n",
        "  for (int row = 0; row < numARows; ++row) {\n",
        "    // B stored in column major!\n",
        "    for (int col = 0; col < numBColumns; ++col) {\n",
        "      // assume row major order for resultRef\n",
        "      resultRef[row * numBColumns + col] = 0.;\n",
        "      for (int k = 0, k < numBRows, ++k) {\n",
        "        resultRef[row * numBColumns + col] += hostA[row * numAColumns + k] * hostB[col * numBColumns + k];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  DataType eps = 1e-5;\n",
        "  for (int i = 0, i < size, ++i) {\n",
        "    if (fabs(hostC[i] - resultRef[i]) < eps) printf(\"Result wrong at index %d\", i);\n",
        "  }\n",
        "\n",
        "\n",
        "  //@@ Free the GPU memory here\n",
        "  cudaFree(deviceA);\n",
        "  cudaFree(deviceB);\n",
        "  cudaFree(deviceC);\n",
        "\n",
        "  //@@ Free the CPU memory here\n",
        "  free(hostA);\n",
        "  free(hostB);\n",
        "  free(hostC);\n",
        "  free(resultRef);\n",
        "  free(kernelTimes);\n",
        "  free(copyToDeviceTimes);\n",
        "  free(copyToHostTimes);\n",
        "\n",
        "  // Free timer memory\n",
        "  cudaEventDestroy(start);\n",
        "  cudaEventDestroy(stop);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "6QgfnlpRHg1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Name three applications domains of matrix multiplication.\n",
        "2. How many floating operations are being performed in your matrix multiply kernel?\n",
        "3. How many global memory reads are being performed by your kernel?  \n",
        "4. For a matrix A of (64x128) and B of (128x64):\n",
        "  - Explain how many CUDA threads and thread blocks you used.\n",
        "  - Profile your program with Nvidia Nsight. What Achieved Occupancy did you get?\n",
        "5. For a matrix A of (1024x1023) and B of (1023x8193):\n",
        "  - Did your program still work? If not, what changes did you make?\n",
        "  - Explain how many CUDA threads and thread blocks you used.\n",
        "  - Profile your program with Nvidia Nsight. What Achieved Occupancy do you get now?\n",
        "6. Further increase the size of matrix A and B, plot a stacked bar chart showing the breakdown of time including (1) data copy from host to device (2) the CUDA kernel (3) data copy from device to host. For this, you will need to add simple CPU timers to your code regions. Explain what you observe.\n",
        "7. Now, change DataType from double to float, re-plot the a stacked bar chart showing the time breakdown. Explain what you observe.\n"
      ],
      "metadata": {
        "id": "LcT1mBOdjhXb"
      }
    }
  ]
}